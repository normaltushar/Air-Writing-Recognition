import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf
from tensorflow.keras.models import load_model
import time
import os

# Initialize MediaPipe Hands with optimized parameters
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

# Initialize video capture
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("Error: Could not open camera")
    exit()

# Get frame dimensions
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
print(f"Camera resolution: {frame_width}x{frame_height}")

# Create hand detector with context manager
with mp_hands.Hands(
    max_num_hands=1,
    model_complexity=1,
    min_detection_confidence=0.8,
    min_tracking_confidence=0.8
) as hands:
    
    # Try to load character recognition model
    model = None
    class_names = []
    try:
        model = load_model('emnist_cnn_model.h5')
        class_names = [str(i) for i in range(10)] + \
                      [chr(i) for i in range(65, 91)] + \
                      [chr(i) for i in range(97, 123)]
        print("Model loaded successfully")
    except Exception as e:
        print(f"Model loading error: {str(e)}")
        print("Using placeholder recognition")

    # Drawing variables
    canvas = np.ones((frame_height, frame_width), dtype=np.uint8) * 255
    index_path = []
    recording = False
    last_point = None
    recognized_text = ""
    last_recognition_time = time.time()
    pinch_threshold = 0.03  # More sensitive pinch detection

    # Create a named window for proper destruction
    cv2.namedWindow('Air Writing Recognition', cv2.WINDOW_NORMAL)
    
    # Initialize FPS tracking
    prev_time = time.time()
    fps = 0
    
    # Main processing loop
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            print("Error: Failed to capture frame")
            break
        
        # Calculate FPS
        current_time = time.time()
        time_diff = current_time - prev_time
        
        # Only calculate FPS if time difference is significant
        if time_diff > 0.001:  # 1 millisecond minimum
            fps = 1 / time_diff
        prev_time = current_time
        
        # Flip frame horizontally for mirror effect
        frame = cv2.flip(frame, 1)
        h, w, _ = frame.shape
        
        # Convert to RGB for MediaPipe
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # Process hands
        results = hands.process(rgb_frame)
        hand_detected = False
        pinch_detected = False
        
        if results.multi_hand_landmarks:
            hand_landmarks = results.multi_hand_landmarks[0]
            hand_detected = True
            
            # Draw hand landmarks with styling
            mp_drawing.draw_landmarks(
                frame,
                hand_landmarks,
                mp_hands.HAND_CONNECTIONS,
                mp_drawing_styles.get_default_hand_landmarks_style(),
                mp_drawing_styles.get_default_hand_connections_style()
            )
            
            # Get finger tip coordinates
            index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]
            thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]
            middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]
            
            # Convert to pixel coordinates
            x, y = int(index_tip.x * w), int(index_tip.y * h)
            tx, ty = int(thumb_tip.x * w), int(thumb_tip.y * h)
            mx, my = int(middle_tip.x * w), int(middle_tip.y * h)
            
            # Draw current finger positions
            cv2.circle(frame, (x, y), 10, (0, 255, 0), -1)  # Green for index
            cv2.circle(frame, (tx, ty), 8, (0, 0, 255), -1)  # Red for thumb
            cv2.circle(frame, (mx, my), 6, (255, 0, 0), -1)   # Blue for middle
            
            # Calculate normalized distance between fingers
            distance = np.sqrt((x - tx)**2 + (y - ty)**2) / w
            
            # Visualize pinch distance
            cv2.putText(frame, f"Dist: {distance:.3f}", (10, 110), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 0), 2)
            
            # Check pinch gesture
            if distance < pinch_threshold:
                cv2.putText(frame, "PINCH DETECTED", (w-200, 30), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                pinch_detected = True
                
                # Toggle recording with debounce
                if not recording:
                    # Start recording
                    recording = True
                    canvas.fill(255)
                    index_path = []
                    print("Recording started")
                else:
                    # Stop recording and process
                    recording = False
                    print("Recording stopped")
                    
                    if len(index_path) > 10:  # Minimum points threshold
                        # Create stroke image
                        stroke_img = np.ones((200, 200), dtype=np.uint8) * 255
                        for i in range(1, len(index_path)):
                            cv2.line(stroke_img, index_path[i-1], index_path[i], 0, 10)
                        
                        # Preprocess for recognition
                        resized = cv2.resize(stroke_img, (28, 28))
                        normalized = resized.astype('float32') / 255.0
                        input_data = np.expand_dims(normalized, axis=(0, -1))
                        
                        # Character recognition
                        if model:
                            predictions = model.predict(input_data, verbose=0)
                            predicted_idx = np.argmax(predictions)
                            confidence = np.max(predictions)
                            
                            if confidence > 0.5:  # Lowered confidence threshold
                                new_char = class_names[predicted_idx]
                                recognized_text += new_char
                                last_recognition_time = time.time()
                                print(f"Recognized: {new_char} (Conf: {confidence:.2f})")
                        else:
                            # Placeholder when model not loaded
                            recognized_text += "X"
                            last_recognition_time = time.time()
            
            # Record finger path if recording
            if recording:
                # Scale coordinates to canvas size
                canvas_x = int(x)
                canvas_y = int(y)
                index_path.append((canvas_x, canvas_y))
                
                # Draw on canvas
                if last_point:
                    cv2.line(canvas, last_point, (canvas_x, canvas_y), 0, 5)
                last_point = (canvas_x, canvas_y)
            else:
                last_point = None
        
        # Display status information
        # Recording status
        status_color = (0, 0, 255) if recording else (0, 255, 0)
        status_text = "RECORDING" if recording else "READY"
        cv2.putText(frame, f"Status: {status_text}", (10, 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2)
        
        # Recognized text
        cv2.putText(frame, f"Text: {recognized_text}", (10, 70), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
        
        # Hand detection status
        hand_status = "Hand: DETECTED" if hand_detected else "Hand: NOT FOUND"
        hand_color = (0, 255, 255) if hand_detected else (0, 0, 255)
        cv2.putText(frame, hand_status, (10, 140), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, hand_color, 2)
        
        # FPS counter
        cv2.putText(frame, f"FPS: {fps:.1f}", (w - 120, 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        
        # Display canvas
        canvas_display = cv2.cvtColor(canvas, cv2.COLOR_GRAY2BGR)
        
        # Combine frame and canvas
        combined = np.hstack([frame, canvas_display])
        
        # Auto-clear after 15 seconds of inactivity
        if (time.time() - last_recognition_time) > 15 and recognized_text:
            recognized_text = ""
            last_recognition_time = time.time()
            print("Text cleared due to inactivity")
        
        # Show combined display
        cv2.imshow('Air Writing Recognition', combined)
        
        # Keyboard controls
        key = cv2.waitKey(1) & 0xFF
        if key == ord(' '):  # Space toggles recording
            recording = not recording
            if recording:
                canvas.fill(255)
                index_path = []
                print("Manual recording started")
            else:
                print("Manual recording stopped")
        elif key == ord('c'):  # Clear text
            recognized_text = ""
            print("Text cleared manually")
        elif key == ord('q'):  # Quit
            print("Quitting application")
            break

# Proper cleanup
cap.release()
cv2.destroyAllWindows()
print("Application closed successfully")
